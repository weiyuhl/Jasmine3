package ai.koog.prompt.executor.clients.bedrock

import ai.koog.prompt.executor.clients.LLModelDefinitions
import ai.koog.prompt.executor.clients.anthropic.AnthropicModels
import ai.koog.prompt.llm.LLMCapability
import ai.koog.prompt.llm.LLMProvider
import ai.koog.prompt.llm.LLModel
import kotlinx.serialization.Serializable

/**
 * Represents a Bedrock model with an optional inference profile prefix.
 *
 * This allows users to explicitly specify an inference profile prefix that will be prepended
 * to the model ID when making requests to AWS Bedrock. If the prefix is `null`, no prefix will
 * be added, and the model ID will be used as-is. If a value is provided, the effective model ID
 * will be "<prefix>.<modelId>".
 *
 * @param model The base LLModel to use.
 * @param modelId The ID of the used model. Defaults to the ID of the provided model.
 * @param inferenceProfilePrefix Optional prefix to prepend to the model ID. If `null`, no prefix is used.
 */
@Serializable
public data class BedrockModel(
    val model: LLModel,
    val modelId: String = model.id,
    val inferenceProfilePrefix: String? = BedrockInferencePrefixes.US.prefix
) {
    /**
     * Returns the effective model ID, only adds inference profile prefix if provided.
     */
    val effectiveModelId: String = inferenceProfilePrefix?.let { "$it.$modelId" } ?: modelId

    /**
     * Returns the LLModel with the effective model ID.
     */
    val effectiveModel: LLModel = model.copy(provider = LLMProvider.Bedrock, id = effectiveModelId)
}

/**
 * AWS Bedrock regions.
 *
 * Represents all available AWS regions where Bedrock service is supported.
 */
@Serializable
public enum class BedrockRegions(public val regionCode: String) {
    US_WEST_1("us-west-1"),
    US_WEST_2("us-west-2"),
    US_EAST_1("us-east-1"),
    US_EAST_2("us-east-2"),
    CA_CENTRAL_1("ca-central-1"),
    CA_WEST_1("ca-west-1"),
    MX_CENTRAL_1("mx-central-1"),
    AF_SOUTH_1("af-south-1"),
    AP_EAST_1("ap-east-1"),
    AP_EAST_2("ap-east-2"),
    AP_NORTHEAST_1("ap-northeast-1"),
    AP_NORTHEAST_2("ap-northeast-2"),
    AP_NORTHEAST_3("ap-northeast-3"),
    AP_SOUTH_1("ap-south-1"),
    AP_SOUTH_2("ap-south-2"),
    AP_SOUTHEAST_1("ap-southeast-1"),
    AP_SOUTHEAST_2("ap-southeast-2"),
    AP_SOUTHEAST_3("ap-southeast-3"),
    AP_SOUTHEAST_4("ap-southeast-4"),
    AP_SOUTHEAST_5("ap-southeast-5"),
    AP_SOUTHEAST_7("ap-southeast-7"),
    EU_CENTRAL_1("eu-central-1"),
    EU_CENTRAL_2("eu-central-2"),
    EU_NORTH_1("eu-north-1"),
    EU_NORTH_2("eu-north-2"),
    EU_WEST_1("eu-west-1"),
    EU_WEST_2("eu-west-2"),
    EU_WEST_3("eu-west-3"),
    IL_CENTRAL_1("il-central-1"),
    ME_CENTRAL_1("me-central-1"),
    ME_SOUTH_1("me-south-1"),
    SA_EAST_1("sa-east-1");

    override fun toString(): String = regionCode
}

/**
 * AWS Bedrock inference ID prefixes.
 *
 */
@Serializable
public enum class BedrockInferencePrefixes(public val prefix: String) {
    US("us"),
    CA("ca"),
    MX("mx"),
    AF("af"),
    AP("ap"),
    EU("eu"),
    IL("il"),
    ME("me"),
    SA("sa");

    override fun toString(): String = prefix
}

/**
 * Bedrock models available through the AWS Bedrock API
 */
public object BedrockModels : LLModelDefinitions {
    // Basic capabilities for text-only models
    private val standardCapabilities: List<LLMCapability> = listOf(
        LLMCapability.Temperature,
        LLMCapability.Completion
    )

    // Tool calling capabilities
    private val toolCapabilities: List<LLMCapability> = listOf(
        LLMCapability.Tools,
        LLMCapability.ToolChoice,
    )

    // Multimodal capabilities (text and images)
    private val multimodalCapabilities: List<LLMCapability> = listOf(
        LLMCapability.Vision.Image,
        LLMCapability.Document
    )

    private val embedCapabilities: List<LLMCapability> = listOf(
        LLMCapability.Embed
    )

    // Full capabilities (multimodal + tools)
    private val fullCapabilities: List<LLMCapability> =
        standardCapabilities + toolCapabilities + multimodalCapabilities

    // Capabilities of the nova models
    private val novaCapabilities: List<LLMCapability> = standardCapabilities + listOf(
        LLMCapability.Tools,
    )

    /**
     * Claude 3 Opus - Anthropic's most powerful model with superior performance on complex tasks
     *
     * This model excels at:
     * - Complex reasoning and analysis
     * - Creative and nuanced content generation
     * - Following detailed instructions
     * - Multimodal understanding (text and images)
     * - Tool/function calling
     */
    public val AnthropicClaude3Opus: LLModel = BedrockModel(
        AnthropicModels.Opus_3.withoutMultimodalCapabilities(),
        "anthropic.claude-3-opus-20240229-v1:0",
    ).effectiveModel

    /**
     * Claude 4 Opus - Anthropic's previous flagship model
     *
     * This model sets new standards in:
     * - Complex reasoning and advanced coding
     * - Autonomous management of complex, multi-step tasks
     * - Extended thinking for deeper reasoning
     * - AI agent capabilities for orchestrating workflows
     * - Multimodal understanding (text and images)
     * - Tool/function calling with parallel execution
     * - Memory capabilities for maintaining continuity
     */
    public val AnthropicClaude4Opus: LLModel = BedrockModel(
        AnthropicModels.Opus_4.withoutMultimodalCapabilities(),
        "anthropic.claude-opus-4-20250514-v1:0",
    ).effectiveModel

    /**
     * Claude 4.1 Opus sets new standards in:
     * - Complex reasoning and advanced coding
     * - Autonomous management of complex, multi-step tasks
     * - Extended thinking for deeper reasoning
     * - AI agent capabilities for orchestrating workflows
     * - Multimodal understanding (text and images)
     * - Tool/function calling with parallel execution
     * - Memory capabilities for maintaining continuity
     */
    public val AnthropicClaude41Opus: LLModel = BedrockModel(
        AnthropicModels.Opus_4_1.withoutMultimodalCapabilities(),
        "anthropic.claude-opus-4-1-20250805-v1:0",
    ).effectiveModel

    /**
     * Claude 4.5 Opus - Anthropic's intelligent, efficient, and the best model in the world for coding, agents,
     * and computer use.
     * It’s also meaningfully better at everyday tasks like deep research and working with slides and spreadsheets.
     * Opus 4.5 is a step forward in what AI systems can do, and a preview of larger changes to how work gets done.
     *
     */
    public val AnthropicClaude45Opus: LLModel = BedrockModel(
        AnthropicModels.Opus_4_5.withoutMultimodalCapabilities(),
        "anthropic.claude-opus-4-5-20251101-v1:0",
    ).effectiveModel

    /**
     * Claude 4 Sonnet - High-performance model with exceptional reasoning and efficiency
     *
     * This model offers:
     * - Superior coding and reasoning capabilities
     * - High-volume use case optimization
     * - Extended thinking mode for complex problems
     * - Task-specific sub-agent functionality
     * - Multimodal understanding (text and images)
     * - Tool/function calling with parallel execution
     * - Precise instruction following
     */
    public val AnthropicClaude4Sonnet: LLModel = BedrockModel(
        AnthropicModels.Sonnet_4.withoutMultimodalCapabilities(),
        "anthropic.claude-sonnet-4-20250514-v1:0",
    ).effectiveModel

    /**
     * Claude 4.5 Sonnet - High-performance model with enhanced capabilities
     *
     * This model offers:
     * - Superior coding and agentic capabilities
     * - Enhanced performance on complex tasks
     * - Improved instruction following
     * - Advanced multimodal understanding (text and images)
     * - Tool/function calling with parallel execution
     * - Optimized for both quality and efficiency
     */
    public val AnthropicClaude4_5Sonnet: LLModel = BedrockModel(
        AnthropicModels.Sonnet_4_5.withoutMultimodalCapabilities(),
        "anthropic.claude-sonnet-4-5-20250929-v1:0",
    ).effectiveModel

    /**
     * Claude Haiku 4.5 - Anthropic's most powerful model for powering real-world agents,
     * with industry-leading capabilities around coding, and computer use.
     *
     * It delivers near-frontier performance for a wide range of use cases, and stands out as
     * one of the best coding and agent models – with the right speed and cost to power free products
     * and high-volume user experiences.
     */
    public val AnthropicClaude4_5Haiku: LLModel = BedrockModel(
        AnthropicModels.Haiku_4_5.withoutMultimodalCapabilities(),
        "anthropic.claude-haiku-4-5-20251001-v1:0",
    ).effectiveModel

    /**
     * Claude 3 Sonnet - Balanced performance model ideal for most use cases
     *
     * This model offers:
     * - Excellent balance of intelligence and speed
     * - Strong performance on reasoning tasks
     * - Multimodal capabilities
     * - Tool/function calling support
     * - Cost-effective for production use
     */
    public val AnthropicClaude3Sonnet: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "anthropic.claude-3-sonnet-20240229-v1:0",
            capabilities = fullCapabilities,
            contextLength = 200_000,
            maxOutputTokens = 4_096,
        ),
    ).effectiveModel

    /**
     * Claude 3.5 Sonnet v2 - Upgraded model with improved intelligence and capabilities
     *
     * This model offers:
     * - Enhanced coding and reasoning capabilities
     * - Improved agentic workflows
     * - Computer use capabilities (beta)
     * - Advanced tool/function calling
     * - Better software development lifecycle support
     * - Multimodal understanding with vision
     */
    public val AnthropicClaude35SonnetV2: LLModel = BedrockModel(
        AnthropicModels.Sonnet_3_5.withoutMultimodalCapabilities(),
        "anthropic.claude-3-5-sonnet-20241022-v2:0",
    ).effectiveModel

    /**
     * Claude 3.5 Haiku - Fast model with improved reasoning capabilities
     *
     * This model combines:
     * - Rapid response times with intelligence
     * - Performance matching Claude 3 Opus on many benchmarks
     * - Strong coding capabilities
     * - Cost-effective for high-volume use cases
     * - Entry-level user-facing products
     * - Specialized sub-agent tasks
     * - Processing large volumes of data
     */
    public val AnthropicClaude35Haiku: LLModel = BedrockModel(
        AnthropicModels.Haiku_3_5.withoutMultimodalCapabilities(),
        "anthropic.claude-3-5-haiku-20241022-v1:0",
    ).effectiveModel

    /**
     * Claude 3 Haiku - Fast and efficient model for high-volume, simple tasks
     *
     * This model is optimized for:
     * - Quick responses
     * - High-volume processing
     * - Basic reasoning and comprehension
     * - Multimodal understanding
     * - Tool/function calling
     */
    public val AnthropicClaude3Haiku: LLModel = BedrockModel(
        AnthropicModels.Haiku_3.withoutMultimodalCapabilities(),
        "anthropic.claude-3-haiku-20240307-v1:0",
    ).effectiveModel

    /**
     * Claude 2.1 - Previous generation Claude model with 200K context window
     *
     * Features:
     * - Extended context window (200K tokens)
     * - Strong reasoning capabilities
     * - Improved accuracy over Claude 2.0
     * - Text-only (no vision support)
     * - No tool calling support
     */
    public val AnthropicClaude21: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "anthropic.claude-v2:1",
            capabilities = standardCapabilities,
            contextLength = 200_000,
        ),
    ).effectiveModel

    /**
     * Claude Instant - Fast, affordable model for simple tasks
     *
     * Optimized for:
     * - Quick responses
     * - Simple Q&A and text tasks
     * - High-volume applications
     * - Cost-sensitive use cases
     */
    public val AnthropicClaudeInstant: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "anthropic.claude-instant-v1",
            capabilities = standardCapabilities,
            contextLength = 100_000,
        ),
    ).effectiveModel

    /**
     * Amazon Nova Micro - Ultra-fast, low-cost model for simple tasks
     *
     * Amazon's most cost-effective model for:
     * - Simple text generation
     * - Basic Q&A tasks
     * - High-volume applications
     * - Quick responses
     *
     * @see <a href="assets.amazon.science/96/7d/0d3e59514abf8fdcfafcdc574300/nova-tech-report-20250317-0810.pdf">
     */
    public val AmazonNovaMicro: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "amazon.nova-micro-v1:0",
            capabilities = novaCapabilities,
            contextLength = 128_000,
        ),
    ).effectiveModel

    /**
     * Amazon Nova Lite - Balanced performance and cost
     *
     * Optimized for:
     * - General text tasks
     * - Moderate complexity reasoning
     * - Cost-sensitive applications
     * - Good balance of speed and quality
     *
     * @see <a href="assets.amazon.science/96/7d/0d3e59514abf8fdcfafcdc574300/nova-tech-report-20250317-0810.pdf">
     */
    public val AmazonNovaLite: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "amazon.nova-lite-v1:0",
            capabilities = novaCapabilities,
            contextLength = 300_000,
        ),
    ).effectiveModel

    /**
     * Amazon Nova Pro - High-performance model for complex tasks
     *
     * Amazon's advanced model for:
     * - Complex reasoning
     * - Long-form content generation
     * - Advanced text understanding
     * - Professional use cases
     *
     * @see <a href="assets.amazon.science/96/7d/0d3e59514abf8fdcfafcdc574300/nova-tech-report-20250317-0810.pdf">
     */
    public val AmazonNovaPro: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "amazon.nova-pro-v1:0",
            capabilities = novaCapabilities,
            contextLength = 300_000,
        ),
    ).effectiveModel

    /**
     * Amazon Nova Premier - Amazon's most capable model
     *
     * Amazon's flagship model for:
     * - Most complex reasoning tasks
     * - Highest quality outputs
     * - Enterprise applications
     * - Mission-critical use cases
     *
     * @see <a href="assets.amazon.science/e5/e6/ccc5378c42dca467d1abe1628ec9/amazon-nova-premier-technical-report-and-model-card.pdf">
     */
    public val AmazonNovaPremier: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "amazon.nova-premier-v1:0",
            capabilities = novaCapabilities,
            contextLength = 1_000_000,
        ),
    ).effectiveModel

    /**
     * Jamba Large - AI21's most powerful hybrid SSM-Transformer model
     *
     * Excels at:
     * - Complex language understanding
     * - Long-form content generation
     * - Reasoning tasks
     * - Following complex instructions
     * - Tool/function calling
     * - Large context windows (up to 256K tokens)
     *
     * @see <a href="huggingface.co/ai21labs/AI21-Jamba-Large-1.5">
     */
    public val AI21JambaLarge: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "ai21.jamba-1-5-large-v1:0",
            capabilities = standardCapabilities + toolCapabilities,
            contextLength = 256_000,
        ),
    ).effectiveModel

    /**
     * Jamba Mini - AI21's efficient hybrid SSM-Transformer model
     *
     * Good for:
     * - General text generation
     * - Moderate complexity tasks
     * - Cost-effective production use
     * - Tool/function calling
     * - Faster inference speeds
     *
     * @see <a href="huggingface.co/ai21labs/AI21-Jamba-Mini-1.5">
     */
    public val AI21JambaMini: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "ai21.jamba-1-5-mini-v1:0",
            capabilities = standardCapabilities + toolCapabilities,
            contextLength = 256_000,
        ),
    ).effectiveModel

    /**
     * Meta's Llama 3 8B Instruct
     *
     * Features:
     * - 8 billion parameters
     * - Llama 3 architecture
     * - Strong instruction following
     * - Efficient performance
     *
     * @see <a href="huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">
     */
    public val MetaLlama3_0_8BInstruct: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "meta.llama3-8b-instruct-v1:0",
            capabilities = standardCapabilities,
            contextLength = 8_000,
        ),
    ).effectiveModel

    /**
     * Meta's Llama 3 70B Instruct
     *
     * Features:
     * - 70 billion parameters
     * - Llama 3 architecture
     * - Superior instruction following
     * - Advanced reasoning
     *
     * @see <a href="huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct">
     */
    public val MetaLlama3_0_70BInstruct: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "meta.llama3-70b-instruct-v1:0",
            capabilities = standardCapabilities,
            contextLength = 8_000,
        ),
    ).effectiveModel

    /**
     * Meta's Llama 3.1 8B Instruct
     *
     * Features:
     * - 8 billion parameters
     * - Llama 3.1 architecture
     * - Strong instruction following
     * - Efficient performance
     *
     * @see <a href="huggingface.co/meta-llama/Llama-3.1-8B-Instruct">
     */
    public val MetaLlama3_1_8BInstruct: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "meta.llama3-1-8b-instruct-v1:0",
            capabilities = standardCapabilities,
            contextLength = 128_000,
        ),
    ).effectiveModel

    /**
     * Meta's Llama 3.1 70B Instruct
     *
     * Features:
     * - 70 billion parameters
     * - Llama 3.1 architecture
     * - Superior instruction following
     * - Advanced reasoning
     *
     * @see <a href="huggingface.co/meta-llama/Llama-3.1-70B-Instruct">
     */
    public val MetaLlama3_1_70BInstruct: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "meta.llama3-1-70b-instruct-v1:0",
            capabilities = standardCapabilities,
            contextLength = 128_000,
        ),
    ).effectiveModel

    /**
     * Meta's Llama 3.1 405B Instruct
     *
     * Features:
     * - 405 billion parameters
     * - Llama 3.1 architecture
     * - Superior instruction following
     * - Advanced reasoning
     *
     * @see <a href="huggingface.co/meta-llama/Llama-3.1-405B-Instruct">
     */
    public val MetaLlama3_1_405BInstruct: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "meta.llama3-1-405b-instruct-v1:0",
            capabilities = standardCapabilities,
            contextLength = 128_000,
        ),
    ).effectiveModel

    /**
     * Meta's Llama 3.2 1B Instruct
     *
     * Features:
     * - 1 billion parameters
     * - Llama 3.2 architecture
     * - Strong instruction following
     * - Efficient performance
     *
     * @see <a href="huggingface.co/meta-llama/Llama-3.2-1B-Instruct">
     */
    public val MetaLlama3_2_1BInstruct: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "meta.llama3-2-1b-instruct-v1:0",
            capabilities = standardCapabilities,
            contextLength = 128_000,
        ),
    ).effectiveModel

    /**
     * Meta's Llama 3.2 3B Instruct
     *
     * Features:
     * - 3 billion parameters
     * - Llama 3.2 architecture
     * - Strong instruction following
     * - Efficient performance
     *
     * @see <a href="huggingface.co/meta-llama/Llama-3.2-3B-Instruct">
     */
    public val MetaLlama3_2_3BInstruct: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "meta.llama3-2-3b-instruct-v1:0",
            capabilities = standardCapabilities,
            contextLength = 128_000,
        ),
    ).effectiveModel

    /**
     * Meta's Llama 3.2 11B Instruct
     *
     * Features:
     * - 11 billion parameters
     * - Llama 3.2 architecture
     * - Strong instruction following
     * - Efficient performance
     *
     * @see <a href="huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct">
     */
    public val MetaLlama3_2_11BInstruct: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "meta.llama3-2-11b-instruct-v1:0",
            capabilities = fullCapabilities,
            contextLength = 128_000,
        ),
    ).effectiveModel

    /**
     * Meta's Llama 3.2 90B Instruct
     *
     * Features:
     * - 90 billion parameters
     * - Llama 3.2 architecture
     * - Superior instruction following
     * - Advanced reasoning
     *
     * @see <a href="huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct">
     */
    public val MetaLlama3_2_90BInstruct: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "meta.llama3-2-90b-instruct-v1:0",
            capabilities = fullCapabilities,
            contextLength = 128_000,
        ),
    ).effectiveModel

    /**
     * Meta's Llama 3.3 70B Instruct
     *
     * Features:
     * - 70 billion parameters
     * - Llama 3.3 architecture
     * - Superior instruction following
     * - Advanced reasoning
     *
     * @see <a href="huggingface.co/meta-llama/Llama-3.3-70B-Instruct">
     */
    public val MetaLlama3_3_70BInstruct: LLModel = BedrockModel(
        LLModel(
            provider = LLMProvider.Bedrock,
            id = "meta.llama3-3-70b-instruct-v1:0",
            capabilities = standardCapabilities,
            contextLength = 128_000,
        ),
    ).effectiveModel

    /**
     * Embedding models available through the AWS Bedrock API.
     *
     * **Note:** Multimodality (image, audio, video) embeddings are currently not supported by the Bedrock client.
     * Only embedding models that take textual input and return embeddings are included in this object.
     *
     * - For up-to-date information on available models, see:
     *   https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html
     *
     * If multimodal support is added to the Bedrock client in the future, embedding models with image/audio/video inputs
     * should be added here and their capabilities extended accordingly.
     */
    public object Embeddings {
        /**
         * Amazon Titan Embeddings G1 - Text
         * Input: Text
         * Output: Embedding
         */
        public val AmazonTitanEmbedText: LLModel = BedrockModel(
            LLModel(
                provider = LLMProvider.Bedrock,
                id = "amazon.titan-embed-text-v1",
                capabilities = embedCapabilities,
                contextLength = 8_192,
            ),
            inferenceProfilePrefix = null
        ).effectiveModel

        /**
         * Amazon Titan Text Embeddings V2
         * Input: Text
         * Output: Embedding
         */
        public val AmazonTitanEmbedTextV2: LLModel = BedrockModel(
            LLModel(
                provider = LLMProvider.Bedrock,
                id = "amazon.titan-embed-text-v2:0",
                capabilities = embedCapabilities,
                contextLength = 8_192,
            ),
            inferenceProfilePrefix = null
        ).effectiveModel

        /**
         * Cohere Embed English v3
         * Input: Text
         * Output: Embedding
         */
        public val CohereEmbedEnglishV3: LLModel = BedrockModel(
            LLModel(
                provider = LLMProvider.Bedrock,
                id = "cohere.embed-english-v3",
                capabilities = embedCapabilities,
                contextLength = 8_192,
            ),
            inferenceProfilePrefix = null
        ).effectiveModel

        /**
         * Cohere Embed Multilingual v3
         * Input: Text
         * Output: Embedding
         */
        public val CohereEmbedMultilingualV3: LLModel = BedrockModel(
            LLModel(
                provider = LLMProvider.Bedrock,
                id = "cohere.embed-multilingual-v3",
                capabilities = embedCapabilities,
                contextLength = 8_192,
            ),
            inferenceProfilePrefix = null
        ).effectiveModel
    }
}

/**
 * Multimodality is currently not supported by Bedrock client.
 * This is a helper function to copy existing model definitions while removing multimodal capabilities.
 */
private fun LLModel.withoutMultimodalCapabilities(): LLModel {
    return copy(
        capabilities = capabilities.filter {
            it !is LLMCapability.Vision &&
                it !is LLMCapability.Audio &&
                it !is LLMCapability.Document
        }
    )
}

/**
 * Extension method to create a new LLModel with a different inference profile prefix.
 * This allows you to easily switch inference profiles for predefined Bedrock models.
 *
 * @param inferencePrefix The inference profile prefix to use (e.g., "eu", "us", "ap")
 * @return A new LLModel with the specified inference profile prefix
 *
 * Example usage:
 * ```kotlin
 * // Use EU inference profile instead of default US
 * val euModel = BedrockModels.AnthropicClaude4Sonnet.withInferenceProfile(BedrockInferencePrefixes.EU.prefix)
 * // euModel.id will be "eu.anthropic.claude-sonnet-4-20250514-v1:0"
 * ```
 */
public fun LLModel.withInferenceProfile(inferencePrefix: String): LLModel {
    require(provider == LLMProvider.Bedrock) {
        "withInferencePrefix() can only be used with Bedrock models, but model provider is $provider"
    }
    val baseModelId = if (id.contains('.')) {
        val potentialPrefix = id.substringBefore('.')
        val validPrefixes = BedrockInferencePrefixes.entries.map { it.prefix }

        if (potentialPrefix in validPrefixes) {
            id.substringAfter('.')
        } else {
            id
        }
    } else {
        id
    }
    return copy(id = "$inferencePrefix.$baseModelId")
}
